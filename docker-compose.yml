# # version: '3.8'

# # services:
# #   mysql:
# #     image: mysql:8.0
# #     environment:
# #       MYSQL_ROOT_PASSWORD: root
# #       MYSQL_DATABASE: airflow_db
# #       MYSQL_USER: airflow
# #       MYSQL_PASSWORD: airflow
# #     ports:
# #       - "3306:3306"
# #     volumes:
# #       - mysql_data:/var/lib/mysql

# #   adminer:
# #     image: adminer
# #     restart: always
# #     ports:
# #       - "8080:8080"

# #   airflow-webserver:
# #     build:
# #       context: .  # This should point to the folder containing your Dockerfile
# #       dockerfile: Dockerfile  # The custom Dockerfile we created above
# #     environment:
# #       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
# #       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://airflow:airflow@mysql:3306/airflow_db"
# #     volumes:
# #       - ./dags:/opt/airflow/dags
# #       - ./logs:/opt/airflow/logs
# #       - ./plugins:/opt/airflow/plugins
# #     ports:
# #       - "8081:8080"
# #     depends_on:
# #       - mysql
# #     command: webserver

# #   airflow-scheduler:
# #     build:
# #       context: .  # Same build context to include MySQL Connector and NewsAPI
# #       dockerfile: Dockerfile
# #     environment:
# #       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
# #       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://airflow:airflow@mysql:3306/airflow_db"
# #     volumes:
# #       - ./dags:/opt/airflow/dags
# #       - ./logs:/opt/airflow/logs
# #       - ./plugins:/opt/airflow/plugins
# #     depends_on:
# #       - mysql
# #     command: scheduler

# #   airflow-init:
# #     build:
# #       context: .  # Same build context for the custom image
# #       dockerfile: Dockerfile
# #     environment:
# #       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
# #       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://airflow:airflow@mysql:3306/airflow_db"
# #     volumes:
# #       - ./dags:/opt/airflow/dags
# #       - ./logs:/opt/airflow/logs
# #       - ./plugins:/opt/airflow/plugins
# #     entrypoint: "airflow db init"
# #     depends_on:
# #       - mysql

# # volumes:
# #   mysql_data:
# #     driver: local


version: '3.8'

services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: airflow_db
      MYSQL_USER: airflow
      MYSQL_PASSWORD: airflow
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql

  adminer:
    image: adminer
    restart: always
    ports:
      - "8080:8080"

  airflow-webserver:
    build:
      context: .  # This should point to the folder containing your Dockerfile
      dockerfile: Dockerfile  # The custom Dockerfile we created above
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://airflow:airflow@mysql:3306/airflow_db'
      AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
      AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      AIRFLOW__WEBSERVER__WORKER_LOG_SERVER_PORT: '8793'
      AIRFLOW__CORE__SECURE_MODE: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'Sg8Dk76ks6GQ9_U71QGRQcUTrFpmxFNLbHkx2j4IMQI'  
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    ports:
      - "8081:8080"
    depends_on:
      - mysql
    command: >
      bash -c "sleep 10 && airflow webserver"

  airflow-scheduler:
    build:
      context: .  # Same build context to include MySQL Connector and NewsAPI
      dockerfile: Dockerfile
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://airflow:airflow@mysql:3306/airflow_db'
      AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
      AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
      AIRFLOW__WEBSERVER__WORKER_LOG_SERVER_PORT: '8793'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'Sg8Dk76ks6GQ9_U71QGRQcUTrFpmxFNLbHkx2j4IMQI'  # Added
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    depends_on:
      - mysql
    command: scheduler

  airflow-init:
    build:
      context: .  # Same build context for the custom image
      dockerfile: Dockerfile
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "mysql+pymysql://airflow:airflow@mysql:3306/airflow_db"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    entrypoint: "airflow db init"
    depends_on:
      - mysql

volumes:
  mysql_data:
    driver: local

# version: '3.8'

# services:
#   mysql:
#     image: mysql:8.0
#     environment:
#       MYSQL_ROOT_PASSWORD: root
#       MYSQL_DATABASE: airflow_db
#       MYSQL_USER: airflow
#       MYSQL_PASSWORD: airflow
#     ports:
#       - "3306:3306"
#     volumes:
#       - mysql_data:/var/lib/mysql

#   adminer:
#     image: adminer
#     restart: always
#     ports:
#       - "8081:8080"

#   airflow-init:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     environment:
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://airflow:airflow@mysql:3306/airflow_db'
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./logs:/opt/airflow/logs
#       - ./plugins:/opt/airflow/plugins
#     entrypoint: "airflow db init"
#     depends_on:
#       - mysql

#   airflow-webserver:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     environment:
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://airflow:airflow@mysql:3306/airflow_db'
#       AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
#       AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
#       AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
#       AIRFLOW__WEBSERVER__WORKER_LOG_SERVER_PORT: '8793'
#       AIRFLOW__CORE__SECURE_MODE: 'false'
#       AIRFLOW__CORE__SECRET_KEY: 'Sg8Dk76ks6GQ9_U71QGRQcUTrFpmxFNLbHkx2j4IMQI'  
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./logs:/opt/airflow/logs
#       - ./plugins:/opt/airflow/plugins
#       - ./scripts:/opt/airflow/scripts
#       - ./data:/opt/airflow/data
#     ports:
#       - "8082:8080"
#     depends_on:
#       - mysql
#       - airflow-init
#     command: webserver

#   airflow-scheduler:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     environment:
#       AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://airflow:airflow@mysql:3306/airflow_db'
#       AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
#       AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
#       AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
#       AIRFLOW__WEBSERVER__WORKER_LOG_SERVER_PORT: '8793'
#       AIRFLOW__CORE__SECURE_MODE: 'false'
#       AIRFLOW__CORE__SECRET_KEY: 'Sg8Dk76ks6GQ9_U71QGRQcUTrFpmxFNLbHkx2j4IMQI'  
#     volumes:
#       - ./dags:/opt/airflow/dags
#       - ./logs:/opt/airflow/logs
#       - ./plugins:/opt/airflow/plugins
#       - ./scripts:/opt/airflow/scripts
#       - ./data:/opt/airflow/data
#     depends_on:
#       - mysql
#       - airflow-init
#     command: scheduler

#   spark-master:
#     image: bitnami/spark:3.4.0
#     environment:
#       SPARK_MODE: master
#     ports:
#       - "7077:7077"
#       - "8083:8080"
#     volumes:
#       - spark_data:/bitnami/spark

#   spark-worker:
#     image: bitnami/spark:3.4.0
#     environment:
#       SPARK_MODE: worker
#       SPARK_MASTER_URL: spark://spark-master:7077
#     depends_on:
#       - spark-master
#     volumes:
#       - spark_data:/bitnami/spark

# volumes:
#   mysql_data:
#     driver: local
#   spark_data:
#     driver: local
